{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle as pc\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "\n",
    "#from seq2seq import *\n",
    "from Encoder import *\n",
    "from Decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBNL_DATA_DIR = 'data/LBNL Building 74/lbnlb74electricity.xlsx'\n",
    "horizon_size = 4\n",
    "best_loss = 100000000.0\n",
    "lr = 0.0001\n",
    "batch_size = 1\n",
    "max_patience = 7\n",
    "max_epochs = 100\n",
    "checkpoint_dir = 'seq2seq_checkpoint'\n",
    "LOG_INTERVAL = 200\n",
    "keep_rate = 0.5\n",
    "enc_unit = 8\n",
    "dec_unit = 8\n",
    "\n",
    "attn = 'bah' # 'bah' , 'luong'\n",
    "\n",
    "debug = False\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    # data load\n",
    "    time, elec = load_data(LBNL_DATA_DIR)\n",
    "    \n",
    "    # modify\n",
    "    elec = missing_value(elec)\n",
    "    print(elec[40226])\n",
    "    \n",
    "    # split data\n",
    "    train, valid, test = split_dataset(elec)\n",
    "    \n",
    "    # build dataset\n",
    "    test_enc_data, test_dec_data = build_dataset(test, horizon_size)\n",
    "    print(\"Build Dataset Finished\")\n",
    "    print(\"----------------------\")\n",
    "    print(\"[Test] enc {}\\tdec {}\".format(len(test_enc_data), len(test_dec_data)))\n",
    "    \n",
    "    encoder = Encoder(enc_unit, batch_size, horizon_size, keep_rate)\n",
    "    decoder = Decoder(dec_unit, batch_size, horizon_size, keep_rate, attn)\n",
    "    # optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=decoder)\n",
    "  \n",
    "    ckpt_dir = os.path.join(checkpoint_dir, 'best_lr-{}_hidden-{}_hr-{}'.format(lr, enc_unit, horizon_size))\n",
    "    latest_ckpt = tf.train.latest_checkpoint(ckpt_dir)    \n",
    "    print(\"latest ckpt: {}\".format(latest_ckpt))\n",
    "    checkpoint.restore(latest_ckpt)\n",
    "    \n",
    "    loss_obj = tf.keras.losses.MeanSquaredError()\n",
    "    rmse_obj = tf.keras.metrics.RootMeanSquaredError()\n",
    "    \n",
    "    test_batches = batch_iter(test_enc_data, test_dec_data, 1)\n",
    "    \n",
    "    test_loss = 0.\n",
    "    test_rmse = 0.\n",
    "\n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(test_batches):\n",
    "        \n",
    "        batch_enc_input = np.array(list(map(lambda x: list(x), batch_x))) # <eos>:1\n",
    "        batch_dec_input = np.array(list(map(lambda x: [2] + list(x), batch_y))) # <sos>: 2\n",
    "        batch_dec_target = np.array(list(map(lambda x: list(x) + [3], batch_y)))\n",
    "        \n",
    "        enc_output, enc_state = encoder(batch_enc_input, training=False)\n",
    "        dec_hidden = enc_state\n",
    "        \n",
    "        batch_loss = 0.\n",
    "        for t in range(batch_dec_target.shape[1]):\n",
    "            if attn == 'no':\n",
    "                pred, dec_hidden = decoder(batch_dec_input[:,t], dec_hidden, enc_output, training=False)            \n",
    "            else:\n",
    "                pred, dec_hidden, _ = decoder(batch_dec_input[:,t], dec_hidden, enc_output, training=False)\n",
    "            \n",
    "            y_true = tf.reshape(batch_dec_target[:,t], (batch_dec_target.shape[0], 1))\n",
    "            \n",
    "            loss = loss_obj(y_true, pred)\n",
    "            batch_loss += tf.reduce_mean(loss)\n",
    "            \n",
    "        batch_loss = batch_loss / int(batch_dec_target.shape[1]) #mse        \n",
    "        batch_rmse = tf.math.sqrt(batch_loss)\n",
    "        \n",
    "        test_loss += batch_loss\n",
    "        test_rmse += batch_rmse\n",
    "        \n",
    "    print(\"test loss: {}\\ttest rmse: {}\".format(test_loss/(batch_idx+1), test_rmse/(batch_idx+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/openpyxl/worksheet/_reader.py:292: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of time, elec: 46111\t46111\n",
      "43.899\n",
      "Size of train, valid, test: 27973\t6994\t11144\n",
      "Build Dataset Finished\n",
      "----------------------\n",
      "[Test] enc 11136\tdec 11136\n",
      "latest ckpt: seq2seq_checkpoint/best_lr-0.0001_hidden-8_hr-4/best_ckpt_34-34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1208 11:04:21.604324 140020881848128 base_layer.py:1772] Layer encoder is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1208 11:04:22.242376 140020881848128 base_layer.py:1772] Layer decoder is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1208 11:11:18.354145 140020881848128 util.py:144] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W1208 11:11:18.354816 140020881848128 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "W1208 11:11:18.355222 140020881848128 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "W1208 11:11:18.355575 140020881848128 util.py:144] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W1208 11:11:18.355917 140020881848128 util.py:144] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "W1208 11:11:18.356232 140020881848128 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 911.3013916015625\ttest rmse: 28.408222198486328\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
