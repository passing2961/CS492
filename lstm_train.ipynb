{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle as pc\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "\n",
    "#from seq2seq import *\n",
    "from LSTM import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBNL_DATA_DIR = 'data/LBNL Building 74/lbnlb74electricity.xlsx'\n",
    "horizon_size = 4\n",
    "best_loss = 100000000.0\n",
    "lr = 0.0001\n",
    "batch_size = 16\n",
    "max_patience = 7\n",
    "max_epochs = 100\n",
    "checkpoint_dir = 'lstm2_checkpoint'\n",
    "LOG_INTERVAL = 200\n",
    "keep_rate = 0.5\n",
    "hidden_unit = 16\n",
    "num_layer = 1\n",
    "\n",
    "debug = False\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # data load\n",
    "    time, elec = load_data(LBNL_DATA_DIR)\n",
    "    \n",
    "    # modify\n",
    "    elec = missing_value(elec)\n",
    "    print(elec[40226])\n",
    "    \n",
    "    elec = std_normalize(elec)\n",
    "    # split data\n",
    "    train, valid, test = split_dataset(elec)\n",
    "    \n",
    "    # build dataset\n",
    "    train_observe, train_pred = lstm_build_dataset(train, horizon_size)\n",
    "    val_observe, val_pred = lstm_build_dataset(valid, horizon_size)\n",
    "    print(\"Build Dataset Finished\")\n",
    "    print(\"[Train] enc {}\\tdec {}\".format(len(train_observe), len(train_pred)))\n",
    "    print(\"[Dev] enc {}\\tdec {}\".format(len(val_observe), len(val_pred)))\n",
    "    \n",
    "    # model\n",
    "    lstm = LSTM(hidden_unit, keep_rate)\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    # loss & metric obj\n",
    "    loss_obj = tf.keras.losses.MeanSquaredError()\n",
    "    rmse_obj = tf.keras.metrics.RootMeanSquaredError()\n",
    "    val_rmse_obj = tf.keras.metrics.RootMeanSquaredError()\n",
    "    \n",
    "    # Earlystopping\n",
    "    earlystopping = EarlyStopping(best_loss, max_patience)\n",
    "    \n",
    "    # ckpt dir\n",
    "    ckpt_dir = os.path.join(checkpoint_dir, 'lr-{}_hidden-{}_hr-{}'.format(lr, hidden_unit, horizon_size))\n",
    "    best_ckpt_dir = os.path.join(checkpoint_dir, 'best_lr-{}_hidden-{}_hr-{}'.format(lr, hidden_unit, horizon_size))\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    if not os.path.exists(best_ckpt_dir):\n",
    "        os.makedirs(best_ckpt_dir)\n",
    "        \n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                     lstm=lstm)\n",
    "    \n",
    "    num_batches_per_epoch = (len(train_observe)-1) // batch_size + 1\n",
    "    print(\"num_batches_per_epoch: {}\".format(num_batches_per_epoch))\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_batches = batch_iter(train_observe, train_pred, batch_size)        \n",
    "        \n",
    "        train_loss = list()\n",
    "        val_loss = list()\n",
    "        \n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_batches):\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                logits = lstm(batch_x, training=True)\n",
    "                \n",
    "                loss = loss_obj(batch_y, logits)\n",
    "                    \n",
    "                trainable_variables = lstm.trainable_variables\n",
    "            \n",
    "            gradients = tape.gradient(loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        \n",
    "            rmse_obj.update_state(batch_y, logits)\n",
    "            train_loss.append(np.mean(loss))\n",
    "\n",
    "            if (batch_idx+1) % LOG_INTERVAL == 0:\n",
    "                print(\"[epoch {} | step {}/{}] loss: {:.4f} (Avg. {:.4f}) RMSE: {:.4f}\".format(epoch+1, \n",
    "                                                                                               batch_idx+1, \n",
    "                                                                                               num_batches_per_epoch, \n",
    "                                                                                               np.mean(loss), \n",
    "                                                                                               np.mean(train_loss), \n",
    "                                                                                               rmse_obj.result().numpy()))\n",
    "        val_batches = batch_iter(val_observe, val_pred, batch_size)\n",
    "        \n",
    "        for val_batch_idx, (val_batch_x, val_batch_y) in enumerate(val_batches):\n",
    "            \n",
    "            val_logits = lstm(val_batch_x, training=False)\n",
    "                   \n",
    "            loss = loss_obj(val_batch_y, val_logits)\n",
    "\n",
    "            val_rmse_obj.update_state(val_batch_y, val_logits)\n",
    "            val_loss.append(np.mean(loss))\n",
    "            \n",
    "        print(\"[epoch {}] loss: {:.4f} RMSE: {:.4f}\".format(epoch + 1, np.mean(val_loss), val_rmse_obj.result().numpy()))\n",
    "\n",
    "        # applying earlystopping\n",
    "        early = earlystopping.update(np.mean(val_loss), epoch)\n",
    "        if early == 'update':\n",
    "            ckpt_prefix = os.path.join(best_ckpt_dir, 'best_ckpt_{}'.format(epoch+1))            \n",
    "            checkpoint.save(file_prefix = ckpt_prefix)\n",
    "            print(\"[epoch {} patience {} max_patience {} best_loss {}]\\tModel best performance!\".format(epoch+1, earlystopping.patience, earlystopping.max_patience, earlystopping.best_loss))\n",
    "        elif early == 'patience':\n",
    "            ckpt_prefix = os.path.join(ckpt_dir, 'ckpt_{}'.format(epoch+1))                    \n",
    "            checkpoint.save(file_prefix = ckpt_prefix)\n",
    "            print(\"[epoch {} patience {} max_patience {} best_loss {}]\\tModel is saved\".format(epoch+1, earlystopping.patience, earlystopping.max_patience, earlystopping.best_loss))\n",
    "        else:\n",
    "            print(\"[epoch {} best_epoch {} patience {} max_patience {} best_loss {}]\\tTraining process is finished\".format(epoch+1, earlystopping.best_epoch+1, earlystopping.patience, earlystopping.max_patience, earlystopping.best_loss))\n",
    "            break\n",
    "        rmse_obj.reset_states()\n",
    "        val_rmse_obj.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/openpyxl/worksheet/_reader.py:292: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of time, elec: 46111\t46111\n",
      "43.899\n",
      "Mean: 31.535117450933615\tStd: 8.103161225783838\n",
      "Size of train, valid, test: 27973\t6994\t11144\n",
      "Build Dataset Finished\n",
      "[Train] enc 27969\tdec 27969\n",
      "[Dev] enc 6990\tdec 6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1208 12:27:15.395750 140153360496448 base_layer.py:1772] Layer lstm is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches_per_epoch: 1749\n",
      "[epoch 1 | step 200/1749] loss: 0.1229 (Avg. 0.3425) RMSE: 0.5852\n",
      "[epoch 1 | step 400/1749] loss: 0.0524 (Avg. 0.2658) RMSE: 0.5155\n",
      "[epoch 1 | step 600/1749] loss: 0.0438 (Avg. 0.2384) RMSE: 0.4882\n",
      "[epoch 1 | step 800/1749] loss: 1.8308 (Avg. 0.2251) RMSE: 0.4745\n",
      "[epoch 1 | step 1000/1749] loss: 0.0660 (Avg. 0.2117) RMSE: 0.4601\n",
      "[epoch 1 | step 1200/1749] loss: 0.0590 (Avg. 0.1989) RMSE: 0.4460\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5021961d5ada>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-b3aa02506d9c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    850\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/KAIST-Lecture/SmartEnergy/project/LSTM.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, training)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#for each in self.layers:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_unit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_unit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'constants'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m   def call(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    850\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcan_use_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m           last_output, outputs, new_h, new_c, runtime = cudnn_lstm(\n\u001b[0;32m--> 961\u001b[0;31m               **cudnn_lstm_kwargs)\n\u001b[0m\u001b[1;32m    962\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcudnn_lstm\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards)\u001b[0m\n\u001b[1;32m   1134\u001b[0m   \u001b[0minit_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m   \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m   \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurrent_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m   \u001b[0;31m# CuDNN has an extra set of bias for inputs, we disable them (setting to 0),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(value, num_or_size_splits, axis, num, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m                 six.integer_types + (tensor_shape.Dimension,)):\n\u001b[1;32m   1694\u001b[0m     return gen_array_ops.split(\n\u001b[0;32m-> 1695\u001b[0;31m         axis=axis, num_split=num_or_size_splits, value=value, name=name)\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msize_splits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(axis, value, num_split, name)\u001b[0m\n\u001b[1;32m   9881\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Split\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9882\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_split\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9883\u001b[0;31m         num_split)\n\u001b[0m\u001b[1;32m   9884\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9885\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
