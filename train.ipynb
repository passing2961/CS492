{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle as pc\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "\n",
    "#from seq2seq import *\n",
    "from Encoder import *\n",
    "from Decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBNL_DATA_DIR = 'data/LBNL Building 74/lbnlb74electricity.xlsx'\n",
    "horizon_size = 96\n",
    "best_loss = 100000000.0\n",
    "lr = 0.0001\n",
    "batch_size = 16\n",
    "max_patience = 7\n",
    "max_epochs = 100\n",
    "checkpoint_dir = 'seq2seq_attn_checkpoint'\n",
    "LOG_INTERVAL = 200\n",
    "keep_rate = 0.5\n",
    "enc_unit = 16\n",
    "dec_unit = 16\n",
    "\n",
    "attn = 'bah' # 'bah' , 'luong'\n",
    "\n",
    "debug = False\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # data load\n",
    "    time, elec = load_data(LBNL_DATA_DIR)\n",
    "    \n",
    "    # modify\n",
    "    elec = missing_value(elec)\n",
    "    print(elec[40226])\n",
    "    \n",
    "    elec = std_normalize(elec)\n",
    "    \n",
    "    # split data\n",
    "    train, valid, test = split_dataset(elec)\n",
    "    \n",
    "    # build dataset\n",
    "    train_enc_data, train_dec_data = build_dataset(train, horizon_size)\n",
    "    val_enc_data, val_dec_data = build_dataset(valid, horizon_size)\n",
    "    test_enc_data, test_dec_data = build_dataset(test, horizon_size)\n",
    "    print(\"Build Dataset Finished\")\n",
    "    print(\"----------------------\")\n",
    "    print(\"[Train] enc {}\\tdec {}\".format(len(train_enc_data), len(train_dec_data)))\n",
    "    print(\"[Dev] enc {}\\tdec {}\".format(len(val_enc_data), len(val_dec_data)))\n",
    "    print(\"[Test] enc {}\\tdec {}\".format(len(test_enc_data), len(test_dec_data)))\n",
    "    \n",
    "    # model\n",
    "    #seq2seq = Seq2Seq(enc_unit, dec_unit, batch_size, horizon_size, dropout_rate)\n",
    "    encoder = Encoder(enc_unit, batch_size, horizon_size, keep_rate)\n",
    "    decoder = Decoder(dec_unit, batch_size, horizon_size, keep_rate, attn)\n",
    "    # optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    # loss & metric obj\n",
    "    loss_obj = tf.keras.losses.MeanSquaredError()\n",
    "    val_loss_obj = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Earlystopping\n",
    "    earlystopping = EarlyStopping(best_loss, max_patience)\n",
    "    \n",
    "    # ckpt dir\n",
    "    ckpt_dir = os.path.join(checkpoint_dir, 'lr-{}_hidden-{}_hr-{}'.format(lr, enc_unit, horizon_size))\n",
    "    best_ckpt_dir = os.path.join(checkpoint_dir, 'best_lr-{}_hidden-{}_hr-{}'.format(lr, enc_unit, horizon_size))\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    if not os.path.exists(best_ckpt_dir):\n",
    "        os.makedirs(best_ckpt_dir)\n",
    "    \n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=decoder)\n",
    "    \n",
    "    num_batches_per_epoch = (len(train_enc_data)-1) // batch_size + 1\n",
    "    print(\"num_batches_per_epoch: {}\".format(num_batches_per_epoch))\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        train_batches = batch_iter(train_enc_data, train_dec_data, batch_size)\n",
    "        \n",
    "        batch_idx = 0\n",
    "        \n",
    "        train_loss = 0.\n",
    "        train_rmse = 0.\n",
    "        val_loss = 0.\n",
    "        val_rmse = 0.\n",
    "        \n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_batches):\n",
    "            \n",
    "            batch_enc_input = np.array(list(map(lambda x: list(x), batch_x))) # <eos>:1\n",
    "            batch_dec_input = np.array(list(map(lambda x: [2] + list(x), batch_y))) # <sos>: 2\n",
    "            batch_dec_target = np.array(list(map(lambda x: list(x) + [3], batch_y)))\n",
    "            \n",
    "            batch_loss = 0.\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                #logits = seq2seq(batch_enc_input, batch_dec_input, True, 'train')\n",
    "                enc_output, enc_state = encoder(batch_enc_input, training=True)\n",
    "                \n",
    "                dec_hidden = enc_state\n",
    "                \n",
    "                for t in range(batch_dec_input.shape[1]):\n",
    "                    if attn == 'no':\n",
    "                        predictions, dec_hidden = decoder(batch_dec_input[:, t], dec_hidden, enc_output, training=True)\n",
    "                    else:\n",
    "                        predictions, dec_hidden, _ = decoder(batch_dec_input[:, t], dec_hidden, enc_output, training=True)\n",
    "                        \n",
    "                    y_true = tf.reshape(batch_dec_target[:,t], (batch_dec_target.shape[0], 1))\n",
    "                    # loss\n",
    "                    loss = loss_obj(y_true, predictions)\n",
    "                    #total_loss.append(loss)\n",
    "                    \n",
    "                    batch_loss += tf.reduce_mean(loss)\n",
    "                    \n",
    "                trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "            \n",
    "            gradients = tape.gradient(batch_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            train_batch_loss = batch_loss / int(batch_dec_input.shape[1])\n",
    "            train_batch_rmse = tf.math.sqrt(train_batch_loss)\n",
    "                                \n",
    "            train_loss += train_batch_loss\n",
    "            train_rmse += train_batch_rmse\n",
    "            #train_losses.update(tf.reduce_mean(total_loss), batch_x.shape[0])\n",
    "            #train_rmses.update(rmse, batch_x.shape[0])\n",
    "\n",
    "            if (batch_idx+1) % LOG_INTERVAL == 0:\n",
    "                print(\"[epoch {} | step {}/{}] loss: {:.4f} (Avg. {:.4f}) RMSE: {:.4f} (Avg. {:.4f})\".format(epoch + 1,\n",
    "                                                                                                            batch_idx+1, num_batches_per_epoch,\n",
    "                                                                                                            train_batch_loss, train_loss/(batch_idx+1),\n",
    "                                                                                                            train_batch_rmse, train_rmse/(batch_idx+1)))\n",
    "\n",
    "        val_batch_idx = 0\n",
    "        \n",
    "        val_batches = batch_iter(val_enc_data, val_dec_data, batch_size)\n",
    "        \n",
    "        for val_batch_idx, (val_batch_x, val_batch_y) in enumerate(val_batches):\n",
    "            \n",
    "            if debug:\n",
    "                print(\"val batch: {}\\n{}\".format(val_batch_x, val_batch_y))\n",
    "\n",
    "                \n",
    "            val_batch_enc_input = np.array(list(map(lambda x: list(x), val_batch_x)), dtype=np.float64)\n",
    "            val_batch_dec_input = np.array(list(map(lambda x: [2] + list(x), val_batch_y)), dtype=np.float64)\n",
    "            val_batch_dec_target = np.array(list(map(lambda x: list(x) + [3], val_batch_y)), dtype=np.float64)\n",
    "            \n",
    "            \n",
    "            if debug:\n",
    "                print(\"val enc: {}\\n\".format(val_batch_enc_input))\n",
    "                print(\"val dec: {}\\n{}\".format(val_batch_dec_input, val_batch_dec_target))\n",
    "                \n",
    "            val_enc_output, val_enc_state = encoder(val_batch_enc_input, training=False)\n",
    "            \n",
    "            val_dec_hidden = val_enc_state\n",
    "            \n",
    "            if debug:\n",
    "                print(\"time step: {}\".format(val_batch_dec_input.shape[1]))\n",
    "                \n",
    "            val_batch_loss = 0.\n",
    "            for t in range(val_batch_dec_input.shape[1]):\n",
    "                \n",
    "                if attn == 'no':\n",
    "                    val_predictions, val_dec_hidden = decoder(val_batch_dec_input[:,t], val_dec_hidden, val_enc_output, training=False)\n",
    "                else:\n",
    "                    val_predictions, val_dec_hidden, _ = decoder(val_batch_dec_input[:,t], val_dec_hidden, val_enc_output, training=False)\n",
    "                if debug:\n",
    "                    print(\"pred: {}\\n\".format(val_predictions))\n",
    "                    print(\"true: {}\".format(val_batch_dec_target[:,t]))\n",
    "                    print(\"val batch_target: {}\\n\".format(val_batch_dec_target[:,t].shape))\n",
    "                \n",
    "                val_y_true = tf.reshape(val_batch_dec_target[:,t], (val_batch_dec_target.shape[0],1))\n",
    "                if debug:\n",
    "                    print(\"val batch_target: {}\\n\".format(val_batch_dec_target[:,t]))\n",
    "                    print(\"val_true: {}\\n\".format(val_y_true))\n",
    "                    \n",
    "                loss = val_loss_obj(val_y_true, val_predictions)\n",
    "                \n",
    "                if debug:\n",
    "                    print(\"loss: {}\\t{}\\n\".format(loss, tf.reduce_mean(loss)))\n",
    "                \n",
    "                val_batch_loss += tf.reduce_mean(loss)\n",
    "                #val_dec_input = tf.expand_dims(val_batch_dec_target[:,t], 1)\n",
    "                #val_total_loss.append(loss)\n",
    "            \n",
    "            if debug:\n",
    "                print(\"batch loss: {}\".format(val_batch_loss/int(val_batch_dec_input.shape[1])))\n",
    "\n",
    "            val_batch_loss = val_batch_loss / int(val_batch_dec_input.shape[1])\n",
    "            val_batch_rmse = tf.math.sqrt(val_batch_loss)\n",
    "            #val_batch_loss = tf.reduce_mean(val_loss)\n",
    "            \n",
    "            val_loss += val_batch_loss\n",
    "            val_rmse += val_batch_rmse\n",
    "            \n",
    "            \n",
    "            if debug:\n",
    "                print(\"loss: {}\\trmse: {}\\tidx: {}\".format(val_loss/(val_batch_idx+1), val_rmse/(val_batch_idx+1), val_batch_idx+1))\n",
    "\n",
    "        print(\"[epoch {}] loss: {:.4f} RMSE: {:.4f}\".format(epoch + 1, val_loss/(val_batch_idx+1), val_rmse/(val_batch_idx+1)))\n",
    "\n",
    "        # applying earlystopping\n",
    "        early = earlystopping.update(val_loss/(val_batch_idx+1), epoch)\n",
    "        if early == 'update':\n",
    "            ckpt_prefix = os.path.join(best_ckpt_dir, 'best_ckpt_{}'.format(epoch+1))            \n",
    "            checkpoint.save(file_prefix = ckpt_prefix)\n",
    "            print(\"[epoch {} patience {} max_patience {} best_loss {}]\\tModel best performance!\".format(epoch+1, earlystopping.patience, earlystopping.max_patience, earlystopping.best_loss))\n",
    "        elif early == 'patience':\n",
    "            ckpt_prefix = os.path.join(ckpt_dir, 'ckpt_{}'.format(epoch+1))                    \n",
    "            checkpoint.save(file_prefix = ckpt_prefix)\n",
    "            print(\"[epoch {} patience {} max_patience {} best_loss {}]\\tModel is saved\".format(epoch+1, earlystopping.patience, earlystopping.max_patience, earlystopping.best_loss))\n",
    "        else:\n",
    "            print(\"[epoch {} best_epoch {} patience {} max_patience {} best_loss {}]\\tTraining process is finished\".format(epoch+1, earlystopping.best_epoch+1, earlystopping.patience, earlystopping.max_patience, earlystopping.best_loss))\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/openpyxl/worksheet/_reader.py:292: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of time, elec: 46111\t46111\n",
      "43.899\n",
      "Mean: 31.535117450933615\tStd: 8.103161225783838\n",
      "Size of train, valid, test: 27973\t6994\t11144\n",
      "Build Dataset Finished\n",
      "----------------------\n",
      "[Train] enc 27781\tdec 27781\n",
      "[Dev] enc 6802\tdec 6802\n",
      "[Test] enc 10952\tdec 10952\n",
      "num_batches_per_epoch: 1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1208 11:56:56.028552 140249027946304 base_layer.py:1772] Layer encoder is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1208 11:56:57.168847 140249027946304 base_layer.py:1772] Layer decoder is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1 | step 200/1737] loss: 0.3855 (Avg. 0.4711) RMSE: 0.6209 (Avg. 0.6809)\n",
      "[epoch 1 | step 400/1737] loss: 0.4993 (Avg. 0.4666) RMSE: 0.7066 (Avg. 0.6780)\n",
      "[epoch 1 | step 600/1737] loss: 0.4821 (Avg. 0.4960) RMSE: 0.6943 (Avg. 0.6957)\n",
      "[epoch 1 | step 800/1737] loss: 1.8262 (Avg. 0.5543) RMSE: 1.3514 (Avg. 0.7211)\n",
      "[epoch 1 | step 1000/1737] loss: 1.3295 (Avg. 0.5563) RMSE: 1.1530 (Avg. 0.7235)\n",
      "[epoch 1 | step 1200/1737] loss: 0.4585 (Avg. 0.5667) RMSE: 0.6771 (Avg. 0.7310)\n",
      "[epoch 1 | step 1400/1737] loss: 0.4922 (Avg. 0.5876) RMSE: 0.7016 (Avg. 0.7432)\n",
      "[epoch 1 | step 1600/1737] loss: 0.3018 (Avg. 0.5663) RMSE: 0.5494 (Avg. 0.7297)\n",
      "[epoch 1] loss: 0.3224 RMSE: 0.5564\n",
      "[epoch 1 patience 0 max_patience 7 best_loss 0.3224213719367981]\tModel best performance!\n",
      "[epoch 2 | step 200/1737] loss: 0.3021 (Avg. 0.4169) RMSE: 0.5497 (Avg. 0.6385)\n",
      "[epoch 2 | step 400/1737] loss: 0.4333 (Avg. 0.4230) RMSE: 0.6582 (Avg. 0.6436)\n",
      "[epoch 2 | step 600/1737] loss: 0.4843 (Avg. 0.4267) RMSE: 0.6959 (Avg. 0.6461)\n",
      "[epoch 2 | step 800/1737] loss: 0.4053 (Avg. 0.4180) RMSE: 0.6366 (Avg. 0.6397)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
